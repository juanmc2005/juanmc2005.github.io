[{"authors":["juan"],"categories":null,"content":"I am a PhD student at Université Paris-Saclay working at LIMSI. My research focuses mainly on continual self-supervised representation learning in neural networks applied to speech and natural language processing. My ultimate goal is to design machine learning systems that can learn throughout their lifetimes like humans do.\nIn my free time I enjoy learning foreign languages, reading good science fiction and sometimes playing the saxophone.\n","date":1588291200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588291200,"objectID":"ff6a91cfff6ac3be2092e597cbe5b072","permalink":"https://juanmc2005.github.io/authors/juan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/juan/","section":"authors","summary":"I am a PhD student at Université Paris-Saclay working at LIMSI. My research focuses mainly on continual self-supervised representation learning in neural networks applied to speech and natural language processing. My ultimate goal is to design machine learning systems that can learn throughout their lifetimes like humans do.","tags":null,"title":"Juan Manuel Coria","type":"authors"},{"authors":["Hervé Bredin","Ruiqing Yin","Juan Manuel Coria","Gregory Gelly","Pavel Korshunov","Marvin Lavechin","Diego Fustes","Hadrien Titeux","Wassim Bouaziz","Marie-Philippe Gill"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"0730914e6f0ef2888bd0359a441aa712","permalink":"https://juanmc2005.github.io/publication/bredin-2020/","publishdate":"2020-04-09T20:48:40.419094Z","relpermalink":"/publication/bredin-2020/","section":"publication","summary":"We introduce pyannote.audio, an open-source toolkit written in Python for speaker diarization. Based on PyTorch machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines. pyannote.audio also comes with pre-trained models covering a wide range of domains for voice activity detection, speaker change detection, overlapped speech detection, and speaker embedding -- reaching state-of-the-art performance for most of them.","tags":null,"title":"pyannote.audio: neural building blocks for speaker diarization","type":"publication"},{"authors":["Juan M. Coria","Hervé Bredin","Sahar Ghannay","Sophie Rosset"],"categories":null,"content":"Quick Usage Apply our best pretrained model with just a couple lines of Python code:\nimport torch model = torch.hub.load('pyannote/pyannote-audio', 'emb') # extract embeddings emb1 = model({'audio': '/path/to/file1.wav'}) emb2 = model({'audio': '/path/to/file2.wav'}) # compute distance between embeddings from scipy.spatial.distance import cdist import numpy as np distance = np.mean(cdist(emb1, emb2, metric='cosine'))   Click the GitHub Repo button above for more details.   ","date":1585612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585612800,"objectID":"347093a9057afa76685cb4caac83925a","permalink":"https://juanmc2005.github.io/publication/coria-2020-comparison/","publishdate":"2020-04-09T20:48:40.418774Z","relpermalink":"/publication/coria-2020-comparison/","section":"publication","summary":"Despite the growing popularity of metric learning approaches, very little work has attempted to perform a fair comparison of these techniques for speaker verification. We try to fill this gap and compare several metric learning loss functions in a systematic manner on the VoxCeleb dataset. The first family of loss functions is derived from the cross entropy loss (usually used for supervised classification) and includes the congenerous cosine loss, the additive angular margin loss, and the center loss. The second family of loss functions focuses on the similarity between training samples and includes the contrastive loss and the triplet loss. We show that the additive angular margin loss function outperforms all other loss functions in the study, while learning more robust representations. Based on a combination of SincNet trainable features and the x-vector architecture, the network used in this paper brings us a step closer to a really-end-to-end speaker verification system, when combined with the additive angular margin loss, while still being competitive with the x-vector baseline. In the spirit of reproducible research, we also release open source Python code for reproducing our results, and share pretrained PyTorch models on torch.hub that can be used either directly or after fine-tuning.","tags":null,"title":"A Comparison of Metric Learning Loss Functions for End-To-End Speaker Verification","type":"publication"}]
[{"authors":["juan"],"categories":null,"content":"I am a PhD student at Université Paris-Saclay working at LIMSI. My research focuses on Continual Representation Learning in neural networks applied to spoken and written language. My goal is to design systems that can learn throughout their lifetimes like humans do.\nIn my free time I enjoy learning foreign languages, reading good science fiction and playing the saxophone.\n","date":1602633600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1602633600,"objectID":"ff6a91cfff6ac3be2092e597cbe5b072","permalink":"https://juanmc2005.github.io/authors/juan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/juan/","section":"authors","summary":"I am a PhD student at Université Paris-Saclay working at LIMSI. My research focuses on Continual Representation Learning in neural networks applied to spoken and written language. My goal is to design systems that can learn throughout their lifetimes like humans do.","tags":null,"title":"Juan Manuel Coria","type":"authors"},{"authors":["Juan Manuel Coria","Hervé Bredin","Sahar Ghannay","Sophie Rosset"],"categories":null,"content":"Note This paper was accepted at SLSP 2020.\nQuick Usage Apply our best pretrained model with just a couple lines of Python code:\nimport torch model = torch.hub.load('pyannote/pyannote-audio', 'emb') # extract embeddings emb1 = model({'audio': '/path/to/file1.wav'}) emb2 = model({'audio': '/path/to/file2.wav'}) # compute distance between embeddings from scipy.spatial.distance import cdist import numpy as np distance = np.mean(cdist(emb1, emb2, metric='cosine'))  ","date":1602633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602633600,"objectID":"347093a9057afa76685cb4caac83925a","permalink":"https://juanmc2005.github.io/publication/coria-2020-comparison/","publishdate":"2020-04-09T20:48:40.418774Z","relpermalink":"/publication/coria-2020-comparison/","section":"publication","summary":"Despite the growing popularity of metric learning approaches, very little work has attempted to perform a fair comparison of these techniques for speaker verification. We try to fill this gap and compare several metric learning loss functions in a systematic manner on the VoxCeleb dataset. The first family of loss functions is derived from the cross entropy loss (usually used for supervised classification) and includes the congenerous cosine loss, the additive angular margin loss, and the center loss. The second family of loss functions focuses on the similarity between training samples and includes the contrastive loss and the triplet loss. We show that the additive angular margin loss function outperforms all other loss functions in the study, while learning more robust representations. Based on a combination of SincNet trainable features and the x-vector architecture, the network used in this paper brings us a step closer to a really-end-to-end speaker verification system, when combined with the additive angular margin loss, while still being competitive with the x-vector baseline. In the spirit of reproducible research, we also release open source Python code for reproducing our results, and share pretrained PyTorch models on torch.hub that can be used either directly or after fine-tuning.","tags":null,"title":"A Comparison of Metric Learning Loss Functions for End-To-End Speaker Verification","type":"publication"},{"authors":["Juan Manuel Coria","Sahar Ghannay","Sophie Rosset","Hervé Bredin"],"categories":[],"content":"","date":1593907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593907200,"objectID":"35084111ed6be2bf595fdb07a752549e","permalink":"https://juanmc2005.github.io/publication/coria-2020-acl/","publishdate":"2020-09-08T11:34:27+02:00","relpermalink":"/publication/coria-2020-acl/","section":"publication","summary":"The task of automatic misogyny identification and categorization has not received as much attention as other natural language tasks have, even though it is crucial for identifying hate speech in social Internet interactions. In this work, we address this sentence classification task from a representation learning perspective, using both a bidirectional LSTM and BERT optimized with the following metric learning loss functions: contrastive loss, triplet loss, center loss, congenerous cosine loss and additive angular margin loss. We set new state-of-the-art for the task with our fine-tuned BERT, whose sentence embeddings can be compared with a simple cosine distance, and we release all our code as open source for easy reproducibility. Moreover, we find that almost every loss function performs equally well in this setting, matching the regular cross entropy loss.","tags":[],"title":"A Metric Learning Approach to Misogyny Categorization","type":"publication"},{"authors":["Hervé Bredin","Ruiqing Yin","Juan Manuel Coria","Gregory Gelly","Pavel Korshunov","Marvin Lavechin","Diego Fustes","Hadrien Titeux","Wassim Bouaziz","Marie-Philippe Gill"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"0730914e6f0ef2888bd0359a441aa712","permalink":"https://juanmc2005.github.io/publication/bredin-2020/","publishdate":"2020-04-09T20:48:40.419094Z","relpermalink":"/publication/bredin-2020/","section":"publication","summary":"We introduce pyannote.audio, an open-source toolkit written in Python for speaker diarization. Based on PyTorch machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines. pyannote.audio also comes with pre-trained models covering a wide range of domains for voice activity detection, speaker change detection, overlapped speech detection, and speaker embedding -- reaching state-of-the-art performance for most of them.","tags":null,"title":"pyannote.audio: neural building blocks for speaker diarization","type":"publication"}]